{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7z5STlqOJTl+HSQVqGxtN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"id":"s5hxm_-HzuIA","executionInfo":{"status":"error","timestamp":1736774417802,"user_tz":-60,"elapsed":6058,"user":{"displayName":"kirbix 94","userId":"13329749952414277843"}},"outputId":"3737d06d-1268-4495-cc4a-4cf6dbee15b7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'x_train' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c40ae5dce236>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#del modelo mejora.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#Evaluación del modelo: una vez que el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"]}],"source":["#Construcción del modelo: tensorFlow utiliza un\n","#modelo de gráficos, lo que significa que primero se\n","#define el modelo y luego se ejecutan. Para definir el\n","#modelo, se puede usar la API Keras, que proporciona\n","#un conjunto de bloques de construcción de alto\n","#nivel para construir el modelo.\n","\n","import tensorflow as tf\n","\n","model = tf.keras.models.Sequential([\n","tf.keras.layers.Flatten(input_shape=(28, 28)),\n","tf.keras.layers.Dense(128, activation=\"relu\"),\n","tf.keras.layers.Dropout(0.2),\n","tf.keras.layers.Dense(10)\n","])\n","\n","#Compilación del modelo: antes de que el modelo\n","#pueda ser entrenado, debe ser compilado. Esto\n","#implica la especificación de la función de pérdida\n","#(que mide qué tan bien el modelo está realizando\n","#durante el entrenamiento), el optimizador (que\n","#determina cómo el modelo se actualiza basado\n","#en la función de pérdida) y cualquier métrica para\n","#monitorear durante el entrenamiento y prueba.\n","\n","model.compile(optimizer=\"adam\",\n","loss=tf.keras.losses.\n","SparseCategoricalCrossentropy(from_logits=True),\n","metrics=[\"accuracy\"])\n","\n","\n","#Entrenamiento del modelo: ahora se puede entrenar\n","#el modelo utilizando los datos de entrenamiento.\n","#El modelo aprenderá a asociar las imágenes y las\n","#etiquetas. Durante el entrenamiento, la precisión\n","#del modelo mejora.\n","\n","model.fit(x_train, y_train, epochs=5)\n","\n","#Evaluación del modelo: una vez que el modelo\n","#ha sido entrenado, se puede evaluar qué tan bien\n","#funciona utilizando los datos de prueba.\n","\n","model.evaluate(x_test, y_test, verbose=2)\n","\n","#Predicciones: ahora que el modelo está entrenado,\n","#se puede usar para hacer predicciones.\n","\n","probability_model = tf.keras.Sequential([\n","model,\n","tf.keras.layers.Softmax()\n","])\n","\n","predictions = probability_model.predict(x_test)\n"]},{"cell_type":"code","source":["# Importamos la biblioteca NumPy\n","import numpy as np\n","\n","# 1. Creación de arrays\n","# Un array en NumPy se puede crear a partir de listas en Python\n","# Ejemplo de un array unidimensional\n","array = np.array([1, 2, 3])  # Esto crea un array con los elementos [1, 2, 3]\n","print(\"Array unidimensional:\", array)\n","\n","# 2. Operaciones matemáticas con arrays\n","# Las operaciones en NumPy son vectorizadas, lo que significa que se aplican a todo el array sin necesidad de bucles\n","result = array * 2  # Multiplica cada elemento del array por 2\n","print(\"Array multiplicado por 2:\", result)\n","\n","# 3. Creación de una matriz (array bidimensional)\n","# Una matriz se puede crear como un array de arrays\n","matrix = np.array([[1, 2], [3, 4], [5, 6]])  # Esto crea una matriz de 3 filas y 2 columnas\n","print(\"Matriz 3x2:\\n\", matrix)\n","\n","# 4. Cambiar la forma de un array (reshape)\n","# NumPy permite reorganizar los datos de un array\n","reshaped = matrix.reshape(2, 3)  # Cambiamos la forma a 2 filas y 3 columnas\n","print(\"Matriz reconfigurada a 2x3:\\n\", reshaped)\n","\n","# 5. Preprocesamiento de datos: Normalización\n","# Normalizar los datos implica escalar los valores para que tengan una media de 0 y una desviación estándar de 1\n","data = np.array([10, 20, 30])  # Datos originales\n","normalized = (data - np.mean(data)) / np.std(data)  # Normalización\n","print(\"Datos normalizados:\", normalized)\n","\n","# 6. Operaciones básicas de selección y filtrado\n","# Selección de elementos en un array\n","print(\"Elemento en la posición [1,1] de la matriz original:\", matrix[1, 1])  # Accede al elemento en la fila 2, columna 2\n","\n","# Filtrar elementos de un array\n","filtered = data[data > 15]  # Selecciona solo los elementos mayores a 15\n","print(\"Elementos mayores a 15:\", filtered)\n","\n","# 7. Interoperabilidad entre NumPy y TensorFlow\n","# Los arrays de NumPy pueden convertirse fácilmente en tensores de TensorFlow y viceversa (requiere la biblioteca TensorFlow)\n","import tensorflow as tf\n","\n","# Convertir un array NumPy a un tensor TensorFlow\n","tensor = tf.convert_to_tensor(array, dtype=tf.float32)\n","print(\"Tensor TensorFlow convertido desde NumPy:\", tensor)\n","\n","# Convertir un tensor TensorFlow de vuelta a un array NumPy\n","array_from_tensor = tensor.numpy()\n","print(\"Array NumPy convertido desde TensorFlow:\", array_from_tensor)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qv3Z6E-nvMeS","executionInfo":{"status":"ok","timestamp":1736789795143,"user_tz":-60,"elapsed":8016,"user":{"displayName":"kirbix 94","userId":"13329749952414277843"}},"outputId":"a3723b85-d312-4209-bd3c-ee910901baf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Array unidimensional: [1 2 3]\n","Array multiplicado por 2: [2 4 6]\n","Matriz 3x2:\n"," [[1 2]\n"," [3 4]\n"," [5 6]]\n","Matriz reconfigurada a 2x3:\n"," [[1 2 3]\n"," [4 5 6]]\n","Datos normalizados: [-1.22474487  0.          1.22474487]\n","Elemento en la posición [1,1] de la matriz original: 4\n","Elementos mayores a 15: [20 30]\n","Tensor TensorFlow convertido desde NumPy: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n","Array NumPy convertido desde TensorFlow: [1. 2. 3.]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","#PREPROCESAMIENTO DE KERAS\n","\n","# -----------------\n","# 1. Preprocesamiento para imágenes\n","# -----------------\n","\n","# Definir las dimensiones de las imágenes de entrada\n","image_height, image_width = 128, 128\n","\n","# Crear un pipeline de preprocesamiento para las imágenes\n","# Este pipeline redimensiona las imágenes y normaliza los valores de los píxeles\n","image_preprocessing = tf.keras.Sequential([\n","    layers.Resizing(image_height, image_width),   # Cambia el tamaño de las imágenes a 128x128 píxeles\n","    layers.Rescaling(1./255)                     # Normaliza los valores de los píxeles al rango [0, 1]\n","])\n","\n","# -----------------\n","# 2. Preprocesamiento para texto\n","# -----------------\n","\n","# Configurar el número máximo de tokens (palabras únicas) y la longitud máxima de las secuencias\n","max_tokens = 10000  # Vocabulario máximo\n","output_sequence_length = 50  # Longitud máxima de las secuencias\n","\n","# Crear una capa de vectorización para procesar texto\n","text_vectorization = layers.TextVectorization(\n","    max_tokens=max_tokens,                # Número máximo de palabras en el vocabulario\n","    output_sequence_length=output_sequence_length  # Longitud máxima de las secuencias de salida\n",")\n","\n","# Adaptar la capa de vectorización a un conjunto de textos de muestra\n","# Esto crea un vocabulario basado en los datos de entrenamiento\n","sample_texts = [\"Esta es una oración de ejemplo\", \"Otra oración para el vectorizador\"]\n","text_vectorization.adapt(sample_texts)  # Aprende el vocabulario y asigna índices a las palabras\n","\n","# -----------------\n","# 3. Modelo que Integra Ambos Flujos\n","# -----------------\n","\n","# Crear la entrada para imágenes, especificando el tamaño esperado (128x128 píxeles con 3 canales para RGB)\n","image_input = layers.Input(shape=(128, 128, 3), name=\"image_input\")\n","\n","# Aplicar el pipeline de preprocesamiento a las imágenes\n","processed_image = image_preprocessing(image_input)\n","\n","# Crear la entrada para texto, especificando que será una cadena de texto\n","text_input = layers.Input(shape=(1,), dtype=tf.string, name=\"text_input\")\n","\n","# Aplicar el vectorizador de texto a la entrada de texto\n","processed_text = text_vectorization(text_input)\n","\n","# Agregar una capa de embedding para convertir las secuencias de texto en representaciones vectoriales densas\n","embedded_text = layers.Embedding(input_dim=max_tokens, output_dim=64)(processed_text)\n","\n","# Reducir las dimensiones del tensor procesado por las imágenes utilizando una capa de pooling global\n","# Esta capa calcula el promedio global sobre todas las dimensiones espaciales (altura y ancho)\n","pool_image = layers.GlobalAveragePooling2D()(processed_image)\n","\n","# Reducir las dimensiones del tensor procesado por el texto utilizando un pooling global sobre las secuencias\n","pool_text = layers.GlobalAveragePooling1D()(embedded_text)\n","\n","# Combinar (concatenar) las salidas procesadas de imágenes y texto en un único tensor\n","combined = layers.Concatenate()([pool_image, pool_text])\n","\n","# Añadir una capa densa (fully connected) para aprender patrones combinados entre texto e imágenes\n","dense_output = layers.Dense(64, activation=\"relu\")(combined)\n","\n","# Añadir una capa de salida final con una única neurona y una activación sigmoide (para clasificación binaria)\n","final_output = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(dense_output)\n","\n","# Construir el modelo final especificando las entradas y salidas\n","model = models.Model(inputs=[image_input, text_input], outputs=final_output)\n","\n","# -----------------\n","# 4. Compilación y Entrenamiento\n","# -----------------\n","\n","# Compilar el modelo especificando:\n","# - Optimizador (Adam) para ajustar los pesos\n","# - Función de pérdida (Binary Crossentropy) para clasificación binaria\n","# - Métrica de evaluación (Accuracy) para monitorear la precisión\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","# -----------------\n","# 5. Datos Simulados\n","# -----------------\n","\n","# Crear datos de ejemplo para imágenes (10 imágenes aleatorias de tamaño 128x128x3)\n","dummy_images = np.random.random((10, 128, 128, 3))\n","\n","# Crear datos de ejemplo para textos (10 oraciones iguales)\n","dummy_texts = tf.constant([\"Esta es una oración\"] * 10, dtype=tf.string)\n","\n","# Crear etiquetas aleatorias (0 o 1) para las 10 muestras\n","dummy_labels = np.random.randint(2, size=(10, 1))\n","\n","# -----------------\n","# 6. Entrenamiento del Modelo\n","# -----------------\n","\n","# Entrenar el modelo usando los datos simulados\n","# - `dummy_images` se pasan como entrada para las imágenes\n","# - `dummy_texts` se pasan como entrada para el texto\n","# - `dummy_labels` se usan como las etiquetas de entrenamiento\n","# - El modelo entrenará durante 5 épocas\n","history = model.fit(\n","    {\"image_input\": dummy_images, \"text_input\": dummy_texts},  # Entradas\n","    dummy_labels,  # Etiquetas\n","    epochs=5  # Número de épocas\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yaJCipAXAK_S","executionInfo":{"status":"ok","timestamp":1737029737318,"user_tz":-60,"elapsed":1734,"user":{"displayName":"kirbix 94","userId":"13329749952414277843"}},"outputId":"cbee32f4-ce01-4204-f319-2c663e420bd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3000 - loss: 0.6977\n","Epoch 2/5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7000 - loss: 0.6866\n","Epoch 3/5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7000 - loss: 0.6776\n","Epoch 4/5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7000 - loss: 0.6700\n","Epoch 5/5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7000 - loss: 0.6636\n"]}]},{"cell_type":"code","source":["#TENSORFLOWS DATASETS\n","\n","import tensorflow_datasets as tfds\n","\n","# Cargar el conjunto de datos CIFAR-10\n","dataset, info = tfds.load('cifar10', split=['train', 'test'], as_supervised=True, with_info=True)\n","\n","# Dividir en entrenamiento y prueba\n","train_dataset = dataset[0].shuffle(1000).batch(32).repeat()\n","test_dataset = dataset[1].batch(32)\n","\n","# Mostrar información sobre los datos\n","print(info)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wBU1jy_AK5O","executionInfo":{"status":"ok","timestamp":1737114798733,"user_tz":-60,"elapsed":240,"user":{"displayName":"kirbix 94","userId":"13329749952414277843"}},"outputId":"d84c531a-ba7b-4a63-e11d-2eff7d34860d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tfds.core.DatasetInfo(\n","    name='cifar10',\n","    full_name='cifar10/3.0.2',\n","    description=\"\"\"\n","    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","    \"\"\",\n","    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n","    data_dir='/root/tensorflow_datasets/cifar10/3.0.2',\n","    file_format=tfrecord,\n","    download_size=Unknown size,\n","    dataset_size=132.40 MiB,\n","    features=FeaturesDict({\n","        'id': Text(shape=(), dtype=string),\n","        'image': Image(shape=(32, 32, 3), dtype=uint8),\n","        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n","    }),\n","    supervised_keys=('image', 'label'),\n","    disable_shuffling=False,\n","    splits={\n","        'test': <SplitInfo num_examples=10000, num_shards=1>,\n","        'train': <SplitInfo num_examples=50000, num_shards=1>,\n","    },\n","    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n","        author = {Alex Krizhevsky},\n","        title = {Learning multiple layers of features from tiny images},\n","        institution = {},\n","        year = {2009}\n","    }\"\"\",\n",")\n"]}]}]}